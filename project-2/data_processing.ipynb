{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1a8d26",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a524d28",
   "metadata": {},
   "source": [
    "### Step 1: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea89bcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This Juputer notebook performs data transformation tasks, including \n",
    "- Converting all message JSON files into a single CSV file\n",
    "- Cleaning and filtering data\n",
    "- Extracting emojis from messages\n",
    "- Adding datetime components\n",
    "- Calculating character and emoji counts\n",
    "- Performing sentiment analysis using VADER\n",
    "\n",
    "The end result is a CSV file of all Instagram messages I have ever sent, enriched with sentiment and count data for analysis.\n",
    "\n",
    "I created additional CSV files for \n",
    "- unigram counts\n",
    "- bigram counts\n",
    "- trigram counts\n",
    "- emoji counts\n",
    "- multi-letter word counts\n",
    "\n",
    "This analysis is done entirely without AI, with the small exception of consulting ChatGPT 5 for emoji unicoding advice and regex pattern syntax.\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c101e65",
   "metadata": {},
   "source": [
    "### Step 2: Parse Messages Data and Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ea6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "inbox_path = 'inbox'\n",
    "all_data = []\n",
    "\n",
    "# Iterate through inbox directories and process message_1.json files\n",
    "for root, dirs, files in os.walk(inbox_path):\n",
    "    for file in files:\n",
    "        if file == 'message_1.json':\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Extract recipient name\n",
    "            recipient_name = os.path.basename(root).split('_')[0]\n",
    "\n",
    "            # Open and read JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = json.load(f)\n",
    "\n",
    "                if isinstance(content, dict):\n",
    "                    if 'messages' in content:\n",
    "                        content = content['messages']\n",
    "                    else:\n",
    "                        content = [content]\n",
    "\n",
    "                # Normalize JSON data into a DataFrame\n",
    "                df = pd.json_normalize(content)\n",
    "                df['recipient_name'] = recipient_name  \n",
    "                all_data.append(df)\n",
    "\n",
    "# Combine all DataFrames and clean data\n",
    "merged_df = pd.concat(all_data, ignore_index=True)\n",
    "merged_df = merged_df[merged_df['sender_name'] == 'Siddharth Chattoraj']\n",
    "merged_df = merged_df[['recipient_name', 'timestamp_ms', 'content']]\n",
    "merged_df.sort_values(by='timestamp_ms', inplace=True)\n",
    "merged_df.to_csv('data/all_messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf216c8f",
   "metadata": {},
   "source": [
    "### Step 3: Clean and Extract Data + Conduct Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf40a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = pd.read_csv('data/all_messages.csv')\n",
    "\n",
    "# Clean emoji encoding issues\n",
    "def fix_emoji(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except Exception:\n",
    "        return text\n",
    "all_messages['content'] = all_messages['content'].apply(fix_emoji)\n",
    "all_messages = all_messages.dropna(subset=['content']).reset_index(drop=True)\n",
    "allowed_chars = r\"a-zA-Z0-9\\s.,!?;â€™:'\\\"()\\[\\]{}\\-\\â€“â€”_@#$/\\\\%&*+=<>~`|^â€¦\\u00C0-\\u017F\"\n",
    "remove_symbols_re = re.compile(fr\"[^{allowed_chars}]\")\n",
    "extract_symbols_re = re.compile(fr\"[^{allowed_chars}]\")\n",
    "\n",
    "all_messages['emojis'] = all_messages['content'].apply(\n",
    "    lambda x: extract_symbols_re.findall(x) if isinstance(x, str) else []\n",
    ")\n",
    "bad_emojis = {'ï¸', 'â€œ', 'â€', 'âƒ£', '\\u200d'}\n",
    "all_messages['emojis'] = all_messages['emojis'].apply(\n",
    "    lambda x: [e for e in x if e not in bad_emojis]\n",
    ")\n",
    "all_messages['content'] = all_messages['content'].apply(\n",
    "    lambda x: ''.join([c for c in x if c not in bad_emojis]) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Fix emoji color issue\n",
    "def restore_color(e):\n",
    "    if not isinstance(e, str):\n",
    "        return e\n",
    "    e = e.replace(\"ðŸ‡¸\", \"\").replace(\"ðŸ‡º\", \"\")\n",
    "    if e in {\"â¤\", \"â˜¹\", \"â˜º\", \"â˜˜\", \"â™¥\"} and not e.endswith(\"\\ufe0f\"):\n",
    "        e = e + \"\\ufe0f\"\n",
    "    return e\n",
    "\n",
    "all_messages['emojis'] = all_messages['emojis'].apply(\n",
    "    lambda lst: [restore_color(e) for e in lst] if isinstance(lst, list) else lst\n",
    ")\n",
    "# drop empty/whitespace emoji tokens in the lists\n",
    "all_messages['emojis'] = all_messages['emojis'].apply(\n",
    "    lambda lst: [e for e in lst if isinstance(e, str) and e.strip()] if isinstance(lst, list) else lst\n",
    ")\n",
    "\n",
    "def restore_color_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return (\n",
    "        s.replace(\"â¤\", \"â¤ï¸\")\n",
    "         .replace(\"â˜¹\", \"â˜¹ï¸\")\n",
    "         .replace(\"â˜º\", \"â˜ºï¸\")\n",
    "         .replace(\"â˜˜\", \"â˜˜ï¸\")\n",
    "         .replace(\"ðŸ‡¸\", \"\")\n",
    "         .replace(\"ðŸ‡º\", \"\")\n",
    "    )\n",
    "\n",
    "all_messages['content'] = all_messages['content'].apply(restore_color_text)\n",
    "\n",
    "# Timestamps to datetime and extract components\n",
    "all_messages['datetime'] = pd.to_datetime(all_messages['timestamp_ms'], unit='ms')\n",
    "all_messages = all_messages.drop(columns=['timestamp_ms'])\n",
    "all_messages = all_messages.sort_values(by='datetime').reset_index(drop=True)\n",
    "all_messages['year'] = all_messages['datetime'].dt.year\n",
    "all_messages['month'] = all_messages['datetime'].dt.month\n",
    "all_messages['day'] = all_messages['datetime'].dt.day\n",
    "all_messages['hour'] = all_messages['datetime'].dt.hour\n",
    "\n",
    "# Add count of number of words and number of emojis\n",
    "all_messages['word_count'] = all_messages['content'].apply(\n",
    "    lambda x: len(x.split()) if isinstance(x, str) else 0\n",
    ")\n",
    "all_messages['character_count'] = all_messages['content'].apply(\n",
    "    lambda x: len(x) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "all_messages['emoji_count'] = all_messages['emojis'].apply(len)\n",
    "\n",
    "# Sentiment Analysis using VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "all_messages['compound_sentiment'] = all_messages['content'].apply(\n",
    "    lambda x: sid.polarity_scores(x)['compound']\n",
    ")\n",
    "\n",
    "def label_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "all_messages['sentiment_label'] = all_messages['compound_sentiment'].apply(label_sentiment)\n",
    "all_messages.to_csv('data/cleaned_messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a003b69",
   "metadata": {},
   "source": [
    "### Step 4: Unigrams, Bigrams, Trigrams, Emojis, and Multi-letter Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d3db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_messages = pd.read_csv('data/cleaned_messages.csv')\n",
    "\n",
    "# Define cleaning\n",
    "url_pattern = r\"(https?://\\S+|www\\.\\S+)\"\n",
    "punctuation_pattern = r\"(^[^\\w']+|[^\\w']+$)\"\n",
    "\n",
    "def clean_for_ngrams(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    tokens = [re.sub(punctuation_pattern, '', t) for t in text.split()]\n",
    "    return ' '.join(t for t in tokens if t)\n",
    "\n",
    "cleaned_messages['content_further'] = cleaned_messages['content'].astype(str).apply(clean_for_ngrams)\n",
    "\n",
    "# Unigrams, Bigrams, and Emoji Frequency Counts\n",
    "unigram_counts = cleaned_messages['content_further'].str.split(expand=True).stack().value_counts()\n",
    "unigram_counts = pd.DataFrame(unigram_counts).rename(columns={0: 'count'}).reset_index().rename(columns={'index': 'unigram'})\n",
    "unigram_counts.to_csv('data/unigram_counts.csv', index=False)\n",
    "\n",
    "bigrams = cleaned_messages['content_further'].apply(lambda x: [' '.join(b) for b in zip(x.split()[:-1], x.split()[1:])])\n",
    "bigram_counts = Counter([b for sublist in bigrams for b in sublist])\n",
    "bigram_counts = pd.Series(bigram_counts).sort_values(ascending=False)\n",
    "bigram_counts = pd.DataFrame(bigram_counts).rename(columns={0: 'count'}).reset_index().rename(columns={'index': 'bigram'})\n",
    "bigram_counts.to_csv('data/bigram_counts.csv', index=False)\n",
    "\n",
    "trigrams = cleaned_messages['content_further'].apply(lambda x: [' '.join(t) for t in zip(x.split()[:-2], x.split()[1:-1], x.split()[2:])])\n",
    "trigram_counts = Counter([t for sublist in trigrams for t in sublist])\n",
    "trigram_counts = pd.Series(trigram_counts).sort_values(ascending=False)\n",
    "trigram_counts = pd.DataFrame(trigram_counts).rename(columns={0: 'count'}).reset_index().rename(columns={'index': 'trigram'})\n",
    "trigram_counts.to_csv('data/trigram_counts.csv', index=False)\n",
    "\n",
    "emoji_counts = pd.Series([e for lst in cleaned_messages['emojis'].dropna() for e in eval(lst) if isinstance(eval(lst), list)]).value_counts()\n",
    "emoji_counts = pd.DataFrame(emoji_counts).rename(columns={0: 'count'}).reset_index().rename(columns={'index': 'emoji'})\n",
    "emoji_counts.to_csv('data/emoji_counts.csv', index=False)\n",
    "\n",
    "# Multi-letter analysis\n",
    "triple_pattern = re.compile(r'(([a-z])\\2{2,})', re.IGNORECASE)\n",
    "\n",
    "def analyze_word(word):\n",
    "    if not isinstance(word, str) or not word:\n",
    "        return None\n",
    "    repeats = [(match.group(1), match.group(2)) for match in triple_pattern.finditer(word)]\n",
    "    if not repeats:\n",
    "        return None\n",
    "    longest_run, repeated_char = max(repeats, key=lambda t: len(t[0]))\n",
    "    return (word, repeated_char.lower(), len(longest_run))\n",
    "\n",
    "words = cleaned_messages['content_further'].fillna('').str.split().explode()\n",
    "analyzed_words = words.apply(analyze_word).dropna()\n",
    "\n",
    "word_data = pd.DataFrame(analyzed_words.tolist(), columns=['word', 'repeated_char', 'duplicate_length'])\n",
    "word_summary = word_data.groupby('word').agg(\n",
    "    occurrences=('word', 'size'),\n",
    "    duplicate_length=('duplicate_length', 'max')\n",
    ").reset_index()\n",
    "\n",
    "max_duplicate_indices = word_data.groupby('word')['duplicate_length'].idxmax()\n",
    "dominant_letters = word_data.loc[max_duplicate_indices, ['word', 'repeated_char']]\n",
    "\n",
    "results = (\n",
    "    word_summary.merge(dominant_letters, on='word', how='left')\n",
    "    .sort_values(['occurrences', 'duplicate_length'], ascending=[False, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "results.to_csv('data/multi_letter_words.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
